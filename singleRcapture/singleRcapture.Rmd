---
documentclass: jss
author:
  - name: Piotr Chlebicki
    affiliation: Stockholm University
    address: |
      | Matematiska institutionen
      | Albano hus 1
      | 106 91 Stockholm, Sweden
    email: \email{piotr.chlebicki@math.su.se}
    url: https://github.com/Kertoo, https://www.su.se/profiles/pich3772
  - name: Maciej Beręsewicz
    orcid: 0000-0002-8281-4301
    email: \email{maciej.beresewicz@ue.poznan.pl}
    affiliation: |
      | Poznań University of Economics and Business 
      | Statistical Office in Poznań
    address: |
      | Poznań University of Economics and Business
      | Department of Statistics
      | Institute of Informatics and Quantitative Economics
      | Al. Niepodległosci 10
      | 61-875 Poznań, Poland
      |
      | Statistical Office in Poznań
      | ul. Wojska Polskiego 27/29
      | 60-624 Poznań, Poland
title:
  formatted: "Single-Source Capture-Recapture Models With \\pkg{singleRcapture}"
  # If you use tex in the formatted title, also supply version without
  plain:     "Single-Source Capture-Recapture Models With singleRcapture"
  # For running headers, if needed
  short:     "\\pkg{singleRcapture}: Single-Source Capture-Recapture Models"
abstract: >
  Estimating population size is an important issue in official statistics, social sciences and natural sciences. One way to approach  
  this problem is to use capture-recapture methods, which can be classified according to the number of sources used, the main
  distinction being between methods based on one source and those based on two or more sources.
  In this presentation we will introduce the \pkg{singleRcapture} R package for fitting SSCR models. The package implements
  state-of-the-art models as well as some new models proposed by the authors (e.g. extensions of zero-truncated one-inflated and
  one-inflated zero-truncated models). The software is intended for users interested in estimating the size of populations,
  particularly those that are difficult to reach or for which information is available from only one source and dual/multiple system
  estimation cannot be used. 

# In this paper, we focus on the former group, i.e. single-source capture-recapture (SSCR). SSCR models assume that observed counts follow truncated count distributions (e.g. zero-truncated Poisson, one-inflated zero-truncated geometric), and this assumption is used to estimate missing (hidden) zero counts. The literature includes applications of SSCR methods for estimating the number of undocumented migrants, cases of domestic violence or homeless people.

keywords:
  # at least one keyword must be supplied
  formatted: [population size estimation, truncated distributuons, count regression models, "\\proglang{R}"]
  plain:     [population size estimation, truncated distributuons, count regression models, R]
preamble: >
  \usepackage{amsmath, amsthm, amssymb}
  \usepackage{calc, ragged2e}
  \DeclareMathAlphabet{\mathmybb}{U}{bbold}{m}{n}
  \newcommand{\1}{\mathcal{I}}
  \newcommand{\bx}{\boldsymbol{x}}
  \newcommand{\bX}{\boldsymbol{X}}
  \newcommand{\bbeta}{\boldsymbol{\beta}}
  \newcommand{\boeta}{\boldsymbol{\eta}}
output:
  rticles::jss_article:
    number_sections: TRUE
    citation_package: natbib
bibliography: refs.bib
biblio-style: jss
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
```

```{css, echo = FALSE}
.bordered {
  border: solid;
}
```

# Introduction

<!---
Maciej tutaj zostawiam tobie
-->

\allowdisplaybreaks
## Literature review

This work is supported by the National Science Center, OPUS 20 grant no. 2020\-/\-39/\-B/\-HS4/\-00941
\textit{Towards census-like statistics for foreign-born populations -- quality, data integration and estimation}

The subject of this workshop is the \pkg{singleRcapture} package and its lightweight extension that allows for integration with other \proglang{R} packages called \pkg{singleRcaptureExtra}.

The package is available on CRAN:
\url{CRAN.R-project.org/package=singleRcapture} while the extension is available on: \url{https://github.com/ncn-foreigners/singleRcaptureExtra}.

The \pkg{singleRcapture} package is an \proglang{R} language package that focuses on implementing state of the art methods for frequentist point and interval estimation of size of closed populations in single-source capture-recapture (SSCR) setting (e.g. estimation of the population size of irregular migrants at set time point in a given area). 

The beginning of inference in single source capture-recapture dates back to the seminal \cite{ztpoisson} paper in which the zero truncated poisson model was applied to study the size of population of irregular migrants in fours cities in Netherlands.


## How do we estimate population size with only one register? The basics of SSCR

<!---
Tu wydaje mi sie że wystarczy
-->

Let $Y_{k}$ represent the number of times $k$-th unit was observed in source data. Clearly, we don not know how often $Y_{k}=0$ and to find the total population size $N$ we need to estimate it. In general, we assume that conditional distribution of $Y_{k}$ given a~vector of covariates $\bx_{k}$ follows some version of zero truncated count data distribution. Knowing the parameters of the distribution we may estimate the population size using Horwitz-Thompson type estimator:
\begin{equation*}
    \hat{N}=
    \sum_{k=1}^{N}\frac{I_{k}}{\mathbb{P}[Y_{k}>0|\bX_{k}]}=
    \sum_{k=1}^{N_{obs}}\frac{1}{\mathbb{P}[Y_{k}>0|\bX_{k}]},
\end{equation*}
where $I_{k}:=\1_{\mathbb{N}}(Y_{k})$, and maximum likelihood estimate of $N$ is obtained after substituting regression estimates for $\mathbb{P}[Y_{k}>0|\bx_{k}]$ into the equation above. Most of the methods relate to poisson processes.

The analytic variance estimation is then done by computing two parts of the decomposition due to the law of total variance:
\begin{equation}\label{law_of_total_variance_decomposition}
  \text{var}[\hat{N}]=
  \mathbb{E}\left[\text{var}
  \left[\hat{N}|I_{1},\dots,I_{n}\right]\right]+
  \text{var}\left[\mathbb{E}[\hat{N}|I_{1},\dots,I_{n}]\right],
\end{equation}
where the first addend is by the multivariate $\delta$ method seen to be:
\begin{equation}
  \mathbb{E}\left[\text{var}
  \left[\hat{N}|I_{1},\dots,I_{n}\right]\right]=
  \left.\left(\frac{\partial(N|I_1,\dots,I_N)}{\partial\boldsymbol{\beta}}\right)^{T}
  \text{cov}\left[\boldsymbol{\beta}\right]
  \left(\frac{\partial(N|I_1,\dots,I_N)}{\partial\boldsymbol{\beta}}\right)
  \right|_{\boldsymbol{\beta}=\hat{\boldsymbol{\beta}}},
\end{equation}
while the later part of the decomposition in \eqref{law_of_total_variance_decomposition} is under the assumption of independence of $I_{k}$'s and after some omitted simplifications one sees that this is optimally estimated via:
\begin{align}
  \text{var}\left(\mathbb{E}(\hat{N}|I_{1},\dots,I_{n})\right)&=
  \text{var}\left(\sum_{k=1}^{N}\frac{I_{k}}{\mathbb{P}(Y_{k}>0)}\right)\nonumber\\
  &\approx\sum_{k=1}^{N_{obs}}\frac{1-\mathbb{P}(Y_{k}>0)}{\mathbb{P}(Y_{k}>0)^{2}},
\end{align}
which forms the basis of confidence interval creation. Confidence intervals are usually constructed under the assumption of (asymptotic) normality of $\hat{N}$ or asymptotic normality of $\ln(\hat{N}-N)$ (or log normality of $\hat{N}$). The latter of which is an attempt to address a common criticism of student type confidence intervals in SSCR, that is a possibly skewed distribution of $\hat{N}$, and results in the confidence interval of the form (for confidence level of $\alpha$):
\begin{equation*}
  \left(N_{obs}+\frac{\hat{N}-N_{obs}}{G},N_{obs}+\left(\hat{N}-N_{obs}\right)G\right),
\end{equation*}
where:
\begin{equation*}
  G = \exp\left(z\left(1-\frac{\alpha}{2}\right)
  \sqrt{\ln\left(1+\frac{\widehat{\text{Var}}(\hat{N})}{\left(\hat{N}-N_{obs}\right)^{2}}\right)}\right).
\end{equation*}


### Existing implementations

<!---
Maciej tutaj zostawiam tobie
-->

There are some packages implementing zero truncated count data models such as \pkg{VGAM} and \pkg{countreg} and they can be integrated within the \pkg{singleRcapture} ecosystem by the lightweight extention \pkg{singleRcaptureExtra}.

# Basic usage

## The \code{estimatePopsize} function {short-title="The" #estimatePopsize-function}

The main function that \pkg{singleRcapture} is built around is \code{estimatePopsize}. The leading design principle was to make using \code{estimatePopsize} as close to standard \code{stats::glm} as possible. The most important arguments are:
\begin{itemize}
    \item \code{formula} -- the main formula (i.e for the Poisson $\lambda$ parameter),
    \item \code{data} -- the \code{data.frame} (or \code{data.frame} coercible) object,
    \item \code{model} -- either a function a string or a family class object specifying which model should be used possible values are listed in documentation. The supplied argument should have the form \code{model =  "ztpoisson"}, \code{model = ztpoisson} or \code{model = ztpoisson(lambdaLink = "log")} the third way is the only one where the user may (but doesn't have to) select a link function.
    \item \code{method} -- numerical method used to fit regression \code{IRLS} or \code{optim},
    \item \code{popVar} -- a method for estimating variance of $\hat{N}$ and confidence interval creation (either bootstrap, analytic or skipping the estimation entirely),
    \item \code{controlMethod, controlModel, controlPopVar} -- control parameters for numerical fitting, specifying additional formulas (inflation, dispersion) and population size estimation respectively. We will tackle these arguments separately,
    \item \code{offset} -- a matrix of offset values with number of columns matching the number of distribution parameters providing offset values to each of linear predictors.
\end{itemize}
With the \code{formula, data, model} being the three arguments which must be provided in \code{estimatePopsize} syntax.

### Example with \proglang{R} code {short-title="Example with R code" #r-code}

The package should be installed from CRAN \url{https://cran.r-project.org/package=singleRcapture} with the usual code:
```{r, eval=FALSE}
install.packages("singleRcapture")
```

To showcase the main function let us recreate the zero truncated Poisson model from \cite{ztpoisson} on the same data included in the package under the name \code{netherlandsimmigrant}:
```{r}
library(singleRcapture)
head(netherlandsimmigrant)
```

This data set contains information about immigrants in four cities (Amsterdam, Rotterdam, The Hague and Utrecht) in Netherlands that have been staying in the country illegally in 1995 and have appeared in police records that year. The number of times each individual appeared in the records is included in the \code{capture} variable with the available covariates being \code{gender, age, reason, nation} being respectively the persons gender and age, reason for being captured and region of the world from which each person comes:
```{r}
summary(netherlandsimmigrant)
```

The basic syntax is indeed vary similar to that of \code{glm} with the output of the summary method being also quite simmilar except for the additional results of the population size estimates:
```{r}
basicModel <- estimatePopsize(
  formula = capture ~ gender + age + nation,
  model   = ztpoisson(),
  data    = netherlandsimmigrant
)
summary(basicModel)
```

### The implementation

### Methods

```{r}
(popEst <- popSizeEst(basicModel))
```
the \code{popEst} object is of the \code{popSizeEstResults} class and \code{list} type and contains the following fields:
\begin{itemize}
  \item a
\end{itemize}

```{r}
dfb <- dfbeta(basicModel)
summary(dfb)
```

```{r}
dfp <- dfpopsize(basicModel, dfbeta = dfb)
summary(dfp)
```

## Marginal frequencies

A popular method of testing the model fit in single source capture-recapture studies is comparing the fitted marginal frequencies $\displaystyle\sum_{j=1}^{N_{obs}}\hat{\mathbb{P}}\left[Y_{j}=k|\bx_{j}, Y_{j} > 0\right]$ with the observed marginal frequencies $\displaystyle\sum_{j=1}^{N}\1_{\{k\}}(Y_{k})=\sum_{j=1}^{N_{obs}}\1_{\{k\}}(Y_{k})$ for $k\geq1$. If a fitted model bears sufficient resemblance to the real data collection process these quantities should be quite close and both $G$ and $\chi^{2}$ tests may be employed in order to test the statistical significance of the discrepancy with the following \pkg{singleRcapture} syntax:
```{r}
margFreq <- marginalFreq(basicModel)
summary(margFreq, df = 1, dropl5 = "group")
```
where the \code{dropl5} argument is used to indicate how to handle the cells with less than $5$ fitted observations, note however that currently there is no continuity correction.

## Plots

The \code{singleRStaticCountData} class has a \code{plot} method implementing several types of quick demonstrative plots such as the rootogram \cite{rootogram} for comparing the fitted and marginal frequencies which we can get with the syntax:
```{r}
plot(basicModel, plotType = "rootogram")
```

```{r}
par(mar = c(2.5, 8.5, 4.1, 2.5), cex.main = .7, cex.lab = .6)
plot(basicModel, plotType = "strata")
dev.off()
```

The full list of plot types along with the list of optional arguments which may be passed from the call to the \code{plot} method down to base \proglang{R} and \pkg{graphics} functions is listed in the help file:
```{r, eval=FALSE}
?plot.singleRStaticCountData
```

# Detailed information

## Fitting method

As previously showcased the \pkg{singleRcapture} package supports modelling (linear) dependence on covariates of all parameters. To that end a modified IRLS algorithm is employed, full details are available in \cite{VGAM-main}. In order to employ the algorithm a modified model matrix is created $\boldsymbol{X}_{\text{vlm}}$ at call to \code{estimatePopsize}. In the context of the models implemented in \pkg{singleRcapture} this matrix can be written as:

\begin{equation}
  \boldsymbol{X}_{vlm}=
  \begin{pmatrix}
    \boldsymbol{X}_{1} & \boldsymbol{0} &\dotso &\boldsymbol{0}\cr
    \boldsymbol{0} & \boldsymbol{X}_{2} &\dotso &\boldsymbol{0}\cr
    \vdots & \vdots & \ddots & \vdots\cr
    \boldsymbol{0} & \boldsymbol{0} &\dotso &\boldsymbol{X}_{p}
  \end{pmatrix}
\end{equation}
where each $\bX_{i}$ corresponds to a model matrix associated with user specified formula.

In the context of multi-parameter families we have a matrix of linear predictors $\boeta$ instead of a vector, with the number of columns matching the number of parameters in the distribution. "Weights" are then modified to be information matrices $\displaystyle\mathbb{E}\left[-\frac{\partial^{2}\ell}{\partial\boeta_{(k)}^{T}\partial\boeta_{(k)}}\right]$ where $\boeta_{(k)}$ is the $k$'th row of $\boeta$, while in the usual IRLS they are scalars $\displaystyle\mathbb{E}\left[-\frac{\partial^{2}\ell}{\partial\eta_{k}^{2}}\right]$ which is often just $\displaystyle-\frac{\partial^{2}\ell}{\partial\eta^{2}}$.

\begin{enumerate}
    \justifying
    \item Initialize with \code{iter}$\leftarrow 1, \boeta\leftarrow$\code{start}$, \boldsymbol{W}\leftarrow I, \ell\leftarrow\ell(\bbeta)$.
    \item Store values from the previous step: $\ell_{-}\leftarrow\ell, \boldsymbol{W}_{-}\leftarrow\boldsymbol{W}, \bbeta_{-}\leftarrow\bbeta$ (the last assignment is omitted during the first iteration), and assign values in current iteration $\displaystyle\boeta\leftarrow\boldsymbol{X}_{\text{vlm}}\bbeta+\boldsymbol{o}, \boldsymbol{W}_{(k)}\leftarrow\mathbb{E}\left[-\frac{\partial^{2}\ell}{\partial\boeta_{(k)}^{T}\partial\boeta_{(k)}}\right], Z\leftarrow\boeta_{(k)}+\frac{\partial\ell}{\partial\boeta_{(k)}}\boldsymbol{W}_{(k)}^{-1}-\boldsymbol{o}_{(k)}$.
    \item Assign current coefficient value: $\bbeta\leftarrow\left(\boldsymbol{X}_{\text{vlm}}\boldsymbol{W}\boldsymbol{X}_{\text{vlm}}\right)^{-1}\boldsymbol{X}_{\text{vlm}}\boldsymbol{W}\boldsymbol{Z}$.
    \item If $\ell(\bbeta)<\ell(\bbeta_{-})$ try selecting the smallest value $h$ such that for $\bbeta_{h}\leftarrow2^{-h}\left(\bbeta+\bbeta_{-}\right)$ the inequality $\ell(\bbeta_{h})>\ell(\bbeta_{-})$ holds if this is successful $\bbeta\leftarrow\bbeta_{h}$ else stop the algorithm.
    \item If convergence is achieved or \code{iter} is higher than \code{maxiter} end algorithm, else \code{iter}$\leftarrow 1+$\code{iter} and return to step 2.
\end{enumerate}


## Avaiable models

The full list of implemented models in \pkg{singleRcapture} along with the expressions for probability density functions and point estimates is found in the collective help file for all family functions:
```{r, eval=FALSE}
?ztpoisson
```

Here we limit ourselves to just listing the family functions:

\begin{itemize}
    \item Zero-truncated and zero-one-truncated Poisson, geometric, NB type II regression where the untruncated distribution is parameterized as:
    \begin{equation*}
        \mathbb{P}[Y=y|\lambda,\alpha] = \frac{\Gamma\left(y+\alpha^{-1}\right)}{\Gamma\left(\alpha^{-1}\right)y!}
        \left(\frac{\alpha^{-1}}{\alpha^{-1}+\lambda}\right)^{\alpha^{-1}}
        \left(\frac{\lambda}{\lambda + \alpha^{-1}}\right)^{y}.
    \end{equation*}
    \item Zero-truncated one-inflated (ztoi) modifications distributions where the new probability $\mathbb{P}^{\ast}$ measure is defined in terms of count data measure $\mathbb{P}$ with support on $\mathbb{N}\cup\{0\}$ as:
    \begin{equation*}
    \mathbb{P}^{\ast}[Y=y]=
    \begin{cases}
    \mathbb{P}[Y=0] & y=0, \\
    \omega\left(1-\mathbb{P}[Y=0]\right)+(1-\omega)\mathbb{P}[Y=1] & y=1, \\
    (1-\omega)\mathbb{P}[Y=y] & y>1,
    \end{cases}
    \end{equation*}
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y|Y>0]=\omega\1_{\{1\}}(y)+(1-\omega)\mathbb{P}[Y=y|Y>0].
    \end{equation*}
    \item One-inflated zero-truncated (oizt) modifications distributions where the new probability $\mathbb{P}^{\ast}$ measure is defined as:
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y] = \omega \1_{\{1\}}(y)+(1-\omega)\mathbb{P}[Y=y],
    \end{equation*}
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y|Y>0] = 
        \omega\frac{\1_{\{1\}}(y)}{1-(1-\omega)\mathbb{P}[Y=0]}+
        (1-\omega)\frac{\mathbb{P}[Y=y]}{1-(1-\omega)\mathbb{P}[Y=0]}.
    \end{equation*}
    \item Generalized Chao's and Zelterman's estimators via logistic regression on variable $Z$ defined as $Z=1$ if $Y=2$ and $Z=0$ if $Y=1$ with $Z\sim b(p)$ where $\text{logit}(p)=\ln(\lambda/2)$ for poisson parameter $\lambda$,
    \begin{align}
        \hat{N} &= N_{obs}+
        \sum_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}
        \left(2\exp\left(\bx_{k}\hat{\bbeta}\right)+
        2\exp\left(2\bx_{k}\hat{\bbeta}\right)\right)^{-1},
        \tag{\text{Chao's estimator}}\\
        \hat{N}&=\sum_{k=1}^{N_{obs}}
        \left(1-\exp\left(-2\exp\left(\bx_{k}\hat{\bbeta}\right)\right)\right)^{-1}.
        \tag{\text{Zelterman's estimator}}
    \end{align}
    \item Alternative approaches to modelling one-inflation that mimic hurdle models where the first type zero truncated hurdle model (ztHurdle) is defined as:
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y]=\begin{cases}
        \frac{\mathbb{P}[Y=0]}{1-\mathbb{P}[Y=1]} & y=0, \\
        \pi(1-\mathbb{P}[Y=1]) & y=1, \\
        (1-\pi) \frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=1]} & y>1,
        \end{cases}
    \end{equation*}
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y|Y>0]=\pi\1_{\{1\}}(y)+
        (1-\pi)\1_{\mathbb{N}\setminus\{1\}}(y)\frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=0]-\mathbb{P}[Y=1]}
    \end{equation*}
    \item The Hurdle zero truncarted (Hurdlezt) is defined as:
    \begin{align*}
        \mathbb{P}^{\ast}[Y=y]&=\begin{cases}
        \pi & y=1, \\
        (1-\pi) \frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=1]} & y\neq1,
        \end{cases}\\
        \mathbb{P}^{\ast}[Y=y|Y>0]&=\begin{cases}
            \pi\frac{1-\mathbb{P}[Y=1]}{1-\mathbb{P}[Y=0]-\mathbb{P}[Y=1]} & y=1,\\
            (1-\pi)\frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=0]-\mathbb{P}[Y=1]} & y>1.
        \end{cases}
    \end{align*}
\end{itemize}

## Structure of a family function

\itemize{
  \item \code{makeMinusLogLike} -- A factory function for creating the:
  \begin{equation*}
    \ell(\boldsymbol{\beta}), 
    \frac{\partial\ell}{\partial\boldsymbol{\beta}},
    \frac{\partial^{2}\ell}{\partial\boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}}
  \end{equation*}
  functions from $\boldsymbol{y}$ vector and $\boldsymbol{X}_{vlm}$ the argument \code{deriv} with possible 
  values in \code{c(0, 1, 2)} provides which derivative to return with the default \code{0} being just the minus log-likelihood.
  \item \code{links} -- List with link functions.
  \item \code{mu.eta, variance} -- Functions of linear predictors that return expected value and variance. There is a `type` argument with 2 possible values \code{"trunc"} and \code{"nontrunc"} that specifies whether to return $\mathbb{E}[Y|Y>0], \text{var}[Y|Y>0]$ or $\mathbb{E}[Y], \text{var}[Y]$ respectively, also the \code{deriv} argument with values in \code{c(0, 1, 2)} is used for indicating the derivative with respect to the linear predictors with is used for providing standard error in \code{predict} method.
  \item \code{family} -- Character that specifies name of the model.
  \item \code{valideta, validmu} -- For now only returns true. In near future will be used to check whether applied linear predictors are valid (i.e. are transformed into some elements of parameter space the subjected to inverse link function).
  \item \code{funcZ, Wfun} -- Functions that create pseudo residuals and working weights used in IRLS algorithm.
  \item \code{devResids} -- Function that given the linear predictors prior weights vector and response vector returns deviance residuals.
  \item \code{pointEst, popVar} -- Functions that given prior weights linear predictors and in the later case also estimation of  $\text{cov}(\hat{\boldsymbol{\beta}})$ and $\boldsymbol{X_{vlm}}$ matrix return point estimate for population size and analytic estimation of its variance.There is a additional boolean parameter \code{contr} in the former function that if set to true returns contribution of each unit.
  \item \code{etaNames} -- Names of linear predictors.
  \item \code{densityFunction} -- A function that given linear predictors returns value of PMF at values \code{x}. Additional argument \code{type} specifies whether to return $\mathbb{P}[Y|Y>0]$ or $\mathbb{P}[Y]$.
  \item \code{simulate} -- A function that generates values of dependent  vector given linear predictors.
  \item \code{getStart} -- Expression for generating starting points.
}

### Implementing custom \pkg{singleRcapture} family function {short-title="Implementing custom singleRcapture family function"}

## Bootstrap algorithms

There are three types of bootstrap algorithms which the user may specify in \code{controlPopVar} controls with \code{bootType} argument which has three possible values \code{"parametric", "semiparametric", "nonparametric"} with the nonparametric being bootstrap being the usual bootstrap algorithm which as argued in \cite{norrpoll} and \cite{zwane}. The idea of semiparametric bootstrap is to modify the usual bootstrap to include the additional uncertainty due to the sample size being a random variable. This type of bootstrap can be in short described as:
\begin{enumerate}
    \item Draw the sample size $N_{obs}'\sim\text{Be}\left(N', \frac{N'-N_{obs}}{N'}\right)$, where $N'=\lfloor\hat{N}\rfloor+b\left(\lfloor\hat{N}\rfloor-\hat{N}\right)$.
    \item Draw $N_{obs}'$ units from the data uniformly without replacement.
    \item Obtain new population size estimate using bootstrap data.
    \item Repeat $1-3$ $B$ times.
\end{enumerate}
In other words we first draw the sample size and then the sample conditional on the sample size. Note that in using semi-parametric bootstrap one implicitly assumes that the population size estimate $\hat{N}$ is accurate. The last implemented bootstrap type is the parametric algorithm which in short first draws the finite population of size $\approx\hat{N}$ from the superpopulation model and then samples from this population according to the selected model:
\begin{enumerate}
    \item Draw the number of covariates equal to $\lfloor\hat{N}\rfloor+b\left(\lfloor\hat{N}\rfloor-\hat{N}\right)$ proportional to the estimated contribution $(\mathbb{P}\left[Y_{k}>0|\bx_{k}\right])^{-1}$ with replacement.
    \item Using the fitted model and regression coefficients $\hat{\bbeta}$ draw for each covariate the $Y$ value from the corresponding probability measure on $\mathbb{N}\cup\{0\}$.
    \item Truncate units with drawn $Y$ value equal to $0$.
    \item Obtain population size estimate based on the truncated data.
    \item Repeat $1-4$ $B$ times.
\end{enumerate}
Note however that for this type of algorithm to result in consistent standard error estimates it is imperative that the estimated model for the entire superpopulation probability space is consistent which may be much less realistic than semiparametric bootstrap. The parametric bootstrap algorithm is the default in \pkg{singleRcapture}.

Additional arguments accepted by the \code{contorlPopVar} function which are relevant to bootstrap are:
\begin{itemize}
  \item \code{alpha, B} -- significance level and number of bootstrap samples to be performed respectively with $0.05$ and $500$ being the default options.
  \item \code{cores} -- number of process cores to use in bootstrap (1 by default) parallel computing is done via \pkg{doParallel, foreach, parallel} packages.
  \item \code{keepbootStat} -- 	logical value indicating whether to keep a vector of statistics produced by bootstrap.
  \item \code{traceBootstrapSize, bootstrapVisualTrace} --  logical values indicating whether sample and population size should be tracked (\code{FALSE} by default) these work only when \code{cores} = 1.
    \item \code{fittingMethod, bootstrapFitcontrol} -- fitting method (by default the same as used in the original call) and control parameters (\code{controlMethod}) for model fitting in bootstrap.
\end{itemize}

# Integration with the \pkg{VGAM, countreg} packages {short-title="Integration with the" #VGAMcountreg-packages}

\newpage

