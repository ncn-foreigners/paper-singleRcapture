---
documentclass: jss
author:
  - name: Piotr Chlebicki
    affiliation: Stockholm University
    address: |
      | Matematiska institutionen
      | Albano hus 1
      | 106 91 Stockholm, Sweden
    email: \email{piotr.chlebicki@math.su.se}
    url: https://github.com/Kertoo, https://www.su.se/profiles/pich3772
    orcid: 0009-0006-4867-7434
  - name: Maciej Beręsewicz
    orcid: 0000-0002-8281-4301
    email: \email{maciej.beresewicz@ue.poznan.pl}
    affiliation: |
      | Poznań University of Economics and Business 
      | Statistical Office in Poznań
    address: |
      | Poznań University of Economics and Business
      | Department of Statistics
      | Institute of Informatics and Quantitative Economics
      | Al. Niepodległosci 10
      | 61-875 Poznań, Poland
      |
      | Statistical Office in Poznań
      | ul. Wojska Polskiego 27/29
      | 60-624 Poznań, Poland
title:
  # If you use tex in the formatted title, also supply version without
  # For running headers, if needed
  formatted: "\\pkg{singleRcapture}: A Package for Single-Source Capture-Recapture Models"
  plain:     "singleRcapture: A Package for Single-Source Capture-Recapture Models"
  short:     "\\pkg{singleRcapture}: Single-Source Capture-Recapture Models"
abstract: >
  Estimating population size is an important issue in official statistics, social sciences and natural sciences. One way to approach  
  this problem is to use capture-recapture methods, which can be classified according to the number of sources used, the most important
  distinction from the prespective of this work being between methods based on one source and those based on two or more sources.
  In this presentation we will introduce the \pkg{singleRcapture} R package for fitting SSCR models. The package implements
  state-of-the-art models as well as some new models proposed by the authors (e.g. extensions of zero-truncated one-inflated and
  one-inflated zero-truncated models). The software is intended for users interested in estimating the size of populations,
  particularly those that are difficult to reach or for which information is available from only one source and dual/multiple system
  estimation cannot be used. 
keywords:
  # at least one keyword must be supplied
  formatted: [population size estimation, hidden populations, truncated distributuons, count regression models, "\\proglang{R}"]
  plain:     [population size estimation, hidden populations, truncated distributuons, count regression models, R]
preamble: >
  \usepackage{amsmath, amsthm, amssymb}
  \usepackage{calc, ragged2e}
  \usepackage[ruled]{algorithm2e}
  \DeclareMathAlphabet{\mathmybb}{U}{bbold}{m}{n}
  \newcommand{\1}{\mathcal{I}}
  \newcommand{\bZero}{\boldsymbol{0}}
  \newcommand{\bx}{\boldsymbol{x}}
  \newcommand{\bX}{\boldsymbol{X}}
  \newcommand{\bbeta}{\boldsymbol{\beta}}
  \newcommand{\boeta}{\boldsymbol{\eta}}
  \newcommand{\bW}{\boldsymbol{W}}
  \newcommand{\bo}{\boldsymbol{o}}
  
output:
  rticles::jss_article:
    number_sections: TRUE
    citation_package: natbib
bibliography: refs.bib
biblio-style: jss
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
```

```{css, echo = FALSE}
.bordered {
  border: solid;
}
```

# Introduction {#sec-introduction}

Population size estimation is a methodological approach employed across
multiple scientific disciplines, serving as a basis for research, policy
formulation, and decision-making processes [@bohning2018capture]. In the
field of statistics, particularly official statistics, precise
population estimates are essential for developing robust economic
models, optimizing resource allocation, and informing evidence-based
policy formulation [cf. @baffour-awuah2009]. Social scientists utilize
advanced population estimation techniques to investigate *hard-to-reach*
populations, such as homeless individuals or illicit drug users, thereby
addressing the inherent limitations of conventional census
methodologies. These techniques are crucial for obtaining accurate data
on populations that are typically under-represented or difficult to
access through traditional sampling methods [@vincent2022estimating]. In
ecology and epidemiology, researchers focus on estimating the size of
specific species or disease-affected populations within defined
geographical regions, which is vital for conservation efforts, ecosystem
management, and public health interventions.

Population size estimation can be approached through various
methodologies, each with distinct advantages and limitations.
Traditional approaches include full enumeration (e.g. census operations)
and comprehensive sample surveys, which, while providing detailed data,
are often resource-intensive and may result in delayed estimates,
particularly for human populations. Alternative methods leverage
existing data sources, such as administrative registers or carefully
designed small-scale studies in wildlife research or census coverage
surveys [@wolter1986some; @zhang2019note]. Application of these sources
often comes with statistical methods, known as *capture-recapture* or
*multiple system estimation*, that utilizes data from multiple
enumerations of the same population [cf. @dunne2024system]. This can be
implemented using a single source with repeated observations, two, or
multiple sources.

In this paper we focus methods that utilize a single data source with
multiple enumerations of the same units [cf. @ztpoisson]. In human
population studies, such data might be derived from police records,
health system databases, or border control logs, while for non-human
populations, veterinary records or specialized field data serve as
analogous sources. These methods are often applied for hard-to-reach or
hidden population where standard sampling methods may be inappropriate
because of the costs or problems with identification of members of these
populations.

While methods for two or more sources are implemented in various
open-source software [e.g., @baillargeon2007rcapture] the single-source
capture-recapture (SSCR) methods are less available being only partially
implemented in existing \proglang{R} packages. The goal of the paper is 
to introduce the \pkg{singleRcapture} and \pkg{singleRcaptureExtra}
packages which by implementing state-of-the-art methods in SSCR and providing
user friendly API which mimics existing \proglang{R} functions (e.g., `glm`)
attemp to bridge this aforementioned gap. In the next subsection we descirbe 
the available \proglang{R} packages that could be used for estimating 
population size based on SSCR methods.

## Software for capture-recapture for single and multiple sources {#sec-software}

Majority of SSCR methods assume zero-truncated distributions or their
extensions (e.g., inclusion of one-inflation). The \pkg{countreg}
[@countreg], \pkg{VGAM} [@VGAM-main] or \pkg{distributions3}
[@distributions3] implement some of those truncated distributions
and the most general distributions such as Generally Altered, Inflated,
Truncated and Deflated (GAITD) can be found in the \pkg{VGAM}. However,
estimation of parameters of a given truncated (and possibly inflated)
distribution is just a first step (similarly as in log-linear models in
capture-recapture with two sources) and to best of our knowledge there
is no open-source software that allows to estimate population size based
on SSCR method, including variance estimator or diagnostics.

Therefore, the goal of the \pkg{singleRcapture} is \proglang{R} language
is to bridge this gap to provide scientists and other practitioners a tool 
for estimation of population size based on SSCR methods. The package implements
state-of-the-art methods as recently described by @bohning2018capture or
@bohning2024one and its extensions (e.g., inclusion of covariates,
different treatment of one-inflation) that we will cover in detail in
Section 1. The package implements variance estimation based on various
methods, allows for implementing custom models as well as diagnostics
plots (e.g. rootograms) with parameters estimated using a modified IRLS
algorithm implemented by us to for estimation stability. Furthermore, as
many \proglang{R} users are familiar with \pkg{countreg} or \pkg{VGAM} we have
implemented a lightweight extension \pkg{singleRcaptureExtra}, available
through Github (\url{https://github.com/ncn-foreigners/singleRcaptureExtra}), 
that allows for integration of \pkg{singleRcapture} with those packages.

The remaining part of the paper is as follows. In Section \ref{sec-theory}
a brief description of the theoretical background is given and information 
on the fitting methods, the available methods and variance estimation is presented. 
In Section \ref{sec-basic} the main functionalities of the package are introduced,
and the main \code{S3} methods as well as implemented diagnostics and useful
functions are covered. 
Section \ref{VGAMcountreg-packages} covers integration with
\pkg{countreg} and \pkg{VGAM} packages through \pkg{singleRcaptureExtra}
package. 
The paper ends with conclusions and an appendix that shows how
to a implement custom model and how one can use the \code{estimatePopsizeFit}
which is faster than the main function but only estimates regression, which
could be of interest to users interested in using any new bootstrap methods
not programmed in the package.

# Theoretical background {#sec-theory}

## How do we estimate population size with a single register?

Let $Y_{k}$ represent the number of times $k$-th unit was observed in a
register. Clearly, we only observe $k:Y_{k}>0$ and we do not know how many
units are missed (i.e. $Y_{k}=0$) and to find the population size
denoted by $N$ we need to estimate it. In general, we assume that
conditional distribution of $Y_{k}$ given a vector of covariates
$\bx_{k}$ follows some version of zero-truncated count data distribution
(and its extensions). Knowing the parameters of the distribution we may
estimate the population size using Horowitz-Thompson type estimator
given by:

\begin{equation}
\hat{N}=
\sum_{k=1}^{N}\frac{I_{k}}{\mathbb{P}[Y_{k}>0|\bX_{k}]}=
\sum_{k=1}^{N_{obs}}\frac{1}{\mathbb{P}[Y_{k}>0|\bX_{k}]},
\label{eq-ht-estimator}
\end{equation}

where $I_{k}:=\1_{\mathbb{N}}(Y_{k})$, and maximum likelihood estimate
of $N$ is obtained after substituting regression estimates for
$\mathbb{P}[Y_{k}>0|\bx_{k}]$ into \eqref{eq-ht-estimator}.

The basic SSCR assumes independence between counts which may be rather
naive as the first capture may significantly influence the behaviour of
a given unit or limit possibilities of further captures (e.g. due to
incarceration). To solve these issues, @godwin2017estimation and
@ztoi-oizt-poisson introduced one-inflated distributions that explicitly
model probability of the singletons by giving additional mass $\omega$
for singletons denoted as $\1_{\{1\}}(y)$:

\begin{equation*}
  \mathbb{P}^{\ast}[Y=y|Y>0] =
  \omega\1_{\{1\}}(y)+(1-\omega)\mathbb{P}[Y=y|Y>0].
\end{equation*}

For more about the one-inflation in the context SSCR see recent review
of @bohning2024one.

The analytic variance estimation is then done by computing two parts of
the decomposition due to the law of total variance given by:

\begin{equation}\label{eq-law_of_total_variance_decomposition}
  \text{var}[\hat{N}] = \mathbb{E}\left[\text{var}
  \left[\hat{N}|I_{1},\dots,I_{n}\right]\right] + 
  \text{var}\left[\mathbb{E}[\hat{N}|I_{1},\dots,I_{n}]\right],
\end{equation} 

where the first part can be estimated using the multivariate $\delta$
method given by:

\begin{equation*}
  \mathbb{E}\left[\text{var} \left[\hat{N}|I_{1},\dots,I_{n}\right]\right] =
  \left.\left(\frac{\partial(N|I_1,\dots,I_N)}{\partial\bbeta}\right)^\top
  \text{cov}\left[\hat{\bbeta}\right]
  \left(\frac{\partial(N|I_1,\dots,I_N)}{\partial\bbeta}\right)
  \right|_{\bbeta=\hat{\bbeta}},
\end{equation*}

while the second part of the decomposition in
\eqref{eq-law_of_total_variance_decomposition} is under the assumption
of independence of $I_{k}$'s and after some omitted simplifications one
sees that this is optimally estimated by:

\begin{equation*}
  \text{var}\left(\mathbb{E}(\hat{N}|I_{1},\dots,I_{n})\right) =
  \text{var}\left(\sum_{k=1}^{N}\frac{I_{k}}{\mathbb{P}(Y_{k}>0)}\right)
  \approx\sum_{k=1}^{N_{obs}}\frac{1-\mathbb{P}(Y_{k}>0)}{\mathbb{P}(Y_{k}>0)^{2}},
\end{equation*}

which forms the basis for the interval estimation. Confidence intervals
are usually constructed under the assumption of (asymptotic) normality
of $\hat{N}$ or asymptotic normality of $\ln(\hat{N}-N)$ (or log
normality of $\hat{N}$). The latter of which is an attempt to address a
common criticism of student type confidence intervals in SSCR, that is a
possibly skewed distribution of $\hat{N}$, and results in the $1-\alpha$
confidence interval given by:

\begin{equation*}
  \left(N_{obs}+\frac{\hat{N}-N_{obs}}{\xi},N_{obs} +
  \left(\hat{N}-N_{obs}\right)\xi\right),
\end{equation*}

where:

\begin{equation*}
  \xi = \exp\left(z\left(1-\frac{\alpha}{2}\right)
  \sqrt{\ln\left(1+\frac{\widehat{\text{Var}}(\hat{N})}{\left(\hat{N}-N_{obs}\right)^{2}}\right)}\right).
\end{equation*}

and where $z$ is the quatile function of the standard normal distribution. 
The estimator $\hat{N}$ is best interpreted as being an estimator for 
the total number of \underline{observable} units in the population since 
we have no means of estimating the number of units in the population 
for which the probability of being included in the data is $0$ \citep[cf.][]{ztpoisson}.

## Available models

The full list of implemented models in \pkg{singleRcapture} along with
the expressions for probability density functions and point estimates can be found
in the collective help file for all family functions:

```{r, eval=FALSE}
?ztpoisson
```

For the sake of simplicity we limit ourselves to just listing the family functions:

\begin{itemize}
    \item Generalized Chao's \citep{chao1987estimating} and Zelterman's \citep{zelterman1988robust} estimators via logistic regression on variable $Z$ defined as $Z=1$ if $Y=2$ and $Z=0$ if $Y=1$ with $Z\sim b(p)$ where $b(\cdot)$ is the Bernoulli distribution and $p$ can be modeled for each unit $k$ by $\text{logit}(p_k)=\ln(\lambda_k/2)$ with Poisson parameter $\lambda_k=\bx_{k}\bbeta$ (for covariate extension see \cite{chao-generalization} and \cite{zelterman}):
    \begin{align}
        \hat{N} &= N_{obs}+
        \sum_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}
        \left(2\exp\left(\bx_{k}\hat{\bbeta}\right)+
        2\exp\left(2\bx_{k}\hat{\bbeta}\right)\right)^{-1},
        \tag{\text{Chao's estimator}}\\
        \hat{N}&=\sum_{k=1}^{N_{obs}}
        \left(1-\exp\left(-2\exp\left(\bx_{k}\hat{\bbeta}\right)\right)\right)^{-1}.
        \tag{\text{Zelterman's estimator}}
    \end{align}
    \item Zero-truncated (\code{zt}$^\ast$) and zero-one-truncated (\code{ztoi}$^\ast$) Poisson \citep[cf. ][]{zotmodels}, geometric, NB type II (NB2) regression where the non-truncated distribution is parameterized as:
    \begin{equation*}
        \mathbb{P}[Y=y|\lambda,\alpha] = \frac{\Gamma\left(y+\alpha^{-1}\right)}{\Gamma\left(\alpha^{-1}\right)y!}
        \left(\frac{\alpha^{-1}}{\alpha^{-1}+\lambda}\right)^{\alpha^{-1}}
        \left(\frac{\lambda}{\lambda + \alpha^{-1}}\right)^{y}.
    \end{equation*}
    \item Zero-truncated one-inflated (\code{ztoi}$^\ast$) modifications distributions where the new probability $\mathbb{P}^{\ast}$ measure is defined in terms of count data measure $\mathbb{P}$ with support on $\mathbb{N}\cup\{0\}$ as:
    \begin{equation*}
    \mathbb{P}^{\ast}[Y=y]=
    \begin{cases}
    \mathbb{P}[Y=0] & y=0, \\
    \omega\left(1-\mathbb{P}[Y=0]\right)+(1-\omega)\mathbb{P}[Y=1] & y=1, \\
    (1-\omega)\mathbb{P}[Y=y] & y>1,
    \end{cases}
    \end{equation*}
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y|Y>0]=\omega\1_{\{1\}}(y)+(1-\omega)\mathbb{P}[Y=y|Y>0].
    \end{equation*}
    \item One-inflated zero-truncated (\code{oizt}$^\ast$) modifications distributions where the new probability $\mathbb{P}^{\ast}$ measure is defined as:
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y] = \omega \1_{\{1\}}(y)+(1-\omega)\mathbb{P}[Y=y],
    \end{equation*}
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y|Y>0] = 
        \omega\frac{\1_{\{1\}}(y)}{1-(1-\omega)\mathbb{P}[Y=0]}+
        (1-\omega)\frac{\mathbb{P}[Y=y]}{1-(1-\omega)\mathbb{P}[Y=0]}.
    \end{equation*}
    Note that \code{ztoi}$^\ast$ and \code{oizt}$^\ast$ distributions are equivalent as shown by \cite{bohning2023equivalence} but population size estimators are different.
\end{itemize}
    
In addition, we have provided two new approaches that allow modelling singletons in a similar was as in Hurdle models. In particular we have proposed the following:

\begin{itemize}
    \item Zero-truncated Hurdle model (\code{ztHurdle}$^\ast$) for Poisson, geometric and NB2 is defined as:
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y]=\begin{cases}
        \frac{\mathbb{P}[Y=0]}{1-\mathbb{P}[Y=1]} & y=0, \\
        \pi(1-\mathbb{P}[Y=1]) & y=1, \\
        (1-\pi) \frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=1]} & y>1,
        \end{cases}
    \end{equation*}
    \begin{equation*}
        \mathbb{P}^{\ast}[Y=y|Y>0]=\pi\1_{\{1\}}(y)+
        (1-\pi)\1_{\mathbb{N}\setminus\{1\}}(y)\frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=0]-\mathbb{P}[Y=1]}.
    \end{equation*}
    \item The Hurdle zero-truncated (\code{Hurdlezt}$^\ast$) for Poisson, geometric and NB2 is defined as:
    \begin{align*}
        \mathbb{P}^{\ast}[Y=y]&=\begin{cases}
        \pi & y=1, \\
        (1-\pi) \frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=1]} & y\neq1,
        \end{cases}\\
        \mathbb{P}^{\ast}[Y=y|Y>0]&=\begin{cases}
            \pi\frac{1-\mathbb{P}[Y=1]}{1-\mathbb{P}[Y=0]-\mathbb{P}[Y=1]} & y=1,\\
            (1-\pi)\frac{\mathbb{P}[Y=y]}{1-\mathbb{P}[Y=0]-\mathbb{P}[Y=1]} & y>1.
        \end{cases}
    \end{align*}
\end{itemize}

The approaches presented above differ in terms of assumptions, computational complexity, or how they treat heterogeneity of captures and singletons. For instance, the dispersion parameter $\alpha$ in the NB2 type models is often interpreted as measuring the \textit{severeness} of unobserved heterogeneity in the underlying poisson process \citep[cf.][]{ztnegbin}. When using any truncated NB model the hope is that due to the class of models considered the consistency is not lost despite the lack of information.

While not discussed in the literature yet the interpretation of heterogeneous $\alpha$ across the population (specified in \code{controlModel}) would be that the unobserved heterogeneity affects the accuracy of the prediction for the dependent variable $Y$ more severely than others. The geometric model (NB with $\alpha=1$) is singled out in the package and often considered in the literature due to inherent computational issues with NB models which are exasperated by the fact that data in SSCR is usually of somewhat low quality. Sparseness of the data is in particular a common issue in SSCR and a big issue for all numerical methods for fitting the (zero-truncated) NB model.

The extra mass $\omega$ in the one-inflated models is an important extension to the researcher's toolbox for SSCR models. Since the inflation at $y=1$ is likely to occur in many types of applications. For example in estimating the number active people who committed criminal acts in a given time period being observed naturally induces a risk of no longer being able to be observed for all units with possibility of arrest. One constraint present in modelling via inflated models is that trying to include both the possibility of one inflation and one deflation leads to both numerical and theoretical problems since the parameter space (of $(\omega, \lambda)$ or $(\omega, \lambda, \alpha)$) is then a much more complicated set.

Hurdle models are another approach to modelling the one-inflation, they can also model deflation as well as both inflation and deflation simultaneously so they are more flexible and situationally the Hurdle zero-truncated models seem to be more numerically stable.

Although interpretation of regression parameters tends to be somewhat overlooked in the SSCR studies we should point out that interpretation of the $\omega$ inflation parameter (in \code{ztoi}$^\ast$ or \code{oizt}$^\ast$) is more convenient that the interpretation of the $\pi$ probability parameter (in Hurdle models). Additionally the interpretation of the $\lambda$ parameter in (one) inflated models conforms to the intuition that given that unit $k$ comes from the non-inflated part of the population then it follows a poisson distribution (respectively geometric or negative binomial) with the $\lambda$ parameter (or $\lambda,\alpha$), in hurdle models one loses that interpretation. It is somewhat interesting is that the estimates from Hurdle zero-truncated and one-inflated zero-truncated models are "usually" quite close to one another, this however require more studies.

## Fitting method

As previously noted the \pkg{singleRcapture} package supports
modelling (linear) dependence on covariates of all parameters. To that
end a modified IRLS algorithm is employed, full details are available in
\cite{VGAM-main}. In order to employ the algorithm a modified model
matrix is created $\bX_{\text{vlm}}$ at call to \code{estimatePopsize}.
In the context of the models implemented in \pkg{singleRcapture} this
matrix can be written as: 

\begin{equation}\label{X_vlm-definition}
  \bX_{vlm}=
  \begin{pmatrix}
    \bX_{1} & \bZero &\dotso &\bZero\cr
    \bZero & \bX_{2} &\dotso &\bZero\cr
    \vdots & \vdots & \ddots & \vdots\cr
    \bZero & \bZero &\dotso &\bX_{p}
  \end{pmatrix}
\end{equation} 

where each $\bX_{i}$ corresponds to a model matrix
associated with user specified formula.

In the context of multi-parameter families we have a matrix of linear
predictors $\boeta$ instead of a vector, with the number of columns
matching the number of parameters in the distribution. "Weights" are
then modified to be information matrices
$\displaystyle\mathbb{E}\left[-\frac{\partial^{2}\ell}{\partial\boeta_{(k)}^\top\partial\boeta_{(k)}}\right]$
where $\boeta_{(k)}$ is the $k$'th row of $\boeta$, while in the usual
IRLS they are scalars
$\displaystyle\mathbb{E}\left[-\frac{\partial^{2}\ell}{\partial\eta_{k}^{2}}\right]$
which is often just
$\displaystyle-\frac{\partial^{2}\ell}{\partial\eta^{2}}$.


\begin{algorithm}[ht!]
\small
\caption{A modified IRLS algorithm used in the \pkg{singleRcapture} package}
\label{algo-estimation}\DontPrintSemicolon
\nlset{1} Initialize with \code{iter}$\leftarrow 1, \boeta\leftarrow$\code{start}
    $, \bW\leftarrow I, \ell\leftarrow\ell(\bbeta)$.\;
\nlset{2} Store values from the previous step: 
    $\ell_{-}\leftarrow\ell, \bW_{-}\leftarrow\bW, \bbeta_{-}\leftarrow\bbeta$ 
    (the last assignment is omitted during the first iteration), and assign values in current iteration 
    $\displaystyle\boeta\leftarrow\bX_{\text{vlm}}\bbeta+\bo, \bW_{(k)}\leftarrow\mathbb{E}\left[-\frac{\partial^{2}\ell}{\partial\boeta_{(k)}^\top\partial\boeta_{(k)}}\right], Z\leftarrow\boeta_{(k)}+\frac{\partial\ell}{\partial\boeta_{(k)}}\bW_{(k)}^{-1}-\bo_{(k)}$.\;
\nlset{3} Assign current coefficient value: 
    $\bbeta\leftarrow\left(\bX_{\text{vlm}}\bW\bX_{\text{vlm}}\right)^{-1}\bX_{\text{vlm}}\bW\boldsymbol{Z}$.\;
\nlset{4} If $\ell(\bbeta)<\ell(\bbeta_{-})$ try selecting the smallest value $h$ such that for
    $\bbeta_{h}\leftarrow2^{-h}\left(\bbeta+\bbeta_{-}\right)$ the inequality $\ell(\bbeta_{h})>\ell(\bbeta_{-})$ 
    holds if this is successful $\bbeta\leftarrow\bbeta_{h}$ else stop the algorithm.\;
\nlset{5} If convergence is achieved or \code{iter} is higher than \code{maxiter} end algorithm, 
    else \code{iter}$\leftarrow 1+$\code{iter} and return to step 2.
\end{algorithm}


## Variance estimation using bootstrap {#sec-boostrap}

We have implemented three types of bootstrap algorithms: parametric, semi-parametric and nonparametric with the
nonparametric being bootstrap being the usual bootstrap algorithm which
as argued in \cite{norrpoll} and \cite{zwane}. 

The idea of semi-parametric bootstrap is to modify the usual bootstrap to include the
additional uncertainty due to the sample size being a random variable.
This type of bootstrap can be in short described as in the Algorithm \ref{algo-semipar-boot}.

\begin{algorithm}[ht!]
\small
\caption{Semi-parametric bootstrap}
\label{algo-semipar-boot}\DontPrintSemicolon
\nlset{1} Draw the sample size $N_{obs}'\sim\text{Be}\left(N', \frac{N_{obs}}{N'}\right)$, where $N'=\lfloor\hat{N}\rfloor+b\left(\lfloor\hat{N}\rfloor-\hat{N}\right)$.\;
\nlset{2} Draw $N_{obs}'$ units from the data uniformly without replacement.\;
\nlset{3} Obtain new population size estimate $N_b$ using bootstrap data.\;
\nlset{4} Repeat $1-3$ $B$ times.
\end{algorithm}

In other words, we first draw the sample size and then the sample
conditional on the sample size. Note that in using semi-parametric
bootstrap one implicitly assumes that the population size estimate
$\hat{N}$ is accurate. The last implemented bootstrap type is the
parametric algorithm which in short first draws the finite population of
size $\approx\hat{N}$ from the superpopulation model and then samples
from this population according to the selected model as described in Algorithm 
\ref{algo-par-boot}.

\begin{algorithm}[ht!]
\small
\caption{Parametric bootstrap}
\label{algo-par-boot}\DontPrintSemicolon
\nlset{1} Draw the number of covariates equal to $\lfloor\hat{N}\rfloor+b\left(\lfloor\hat{N}\rfloor-\hat{N}\right)$ proportional to the estimated contribution $(\mathbb{P}\left[Y_{k}>0|\bx_{k}\right])^{-1}$ with replacement.\;
\nlset{2} Using the fitted model and regression coefficients $\hat{\bbeta}$ draw for each covariate the $Y$ value from the corresponding probability measure on $\mathbb{N}\cup\{0\}$.\;
\nlset{3} Truncate units with drawn $Y$ value equal to $0$.\;
\nlset{4} Obtain population size estimate $N_b$ based on the truncated data.\;
\nlset{5} Repeat $1-3$ $B$ times.
\end{algorithm}

Note that for this type of algorithm to result in consistent
standard error estimates it is imperative that the estimated model for
the entire superpopulation probability space is consistent which may be
much less realistic than semi-parametric bootstrap. The parametric
bootstrap algorithm is the default in \pkg{singleRcapture}.


# Basic usage {#sec-basic}

## The main function

The main function that \pkg{singleRcapture} is built around is
\code{estimatePopsize}. The leading design principle was to make using
\code{estimatePopsize} as close to standard \code{stats::glm} as
possible. The most important arguments are:

\begin{itemize}
    \item \code{formula} -- the main formula (i.e for the Poisson $\lambda$ parameter),
    \item \code{data} -- the \code{data.frame} (or \code{data.frame} coercible) object,
    \item \code{model} -- either a function a string or a family class object specifying which model should be used possible values are listed in documentation. The supplied argument should have the form \code{model =  "ztpoisson"}, \code{model = ztpoisson} or if link function should be specified then \code{model = ztpoisson(lambdaLink = "log")} can be used,
    \item \code{method} -- numerical method used to fit regression \code{IRLS} or \code{optim},
    \item \code{popVar} -- a method for estimating variance of $\hat{N}$ and confidence interval creation (either bootstrap, analytic or skipping the estimation entirely),
    \item \code{controlMethod, controlModel, controlPopVar} -- control parameters for numerical fitting, specifying additional formulas (inflation, dispersion) and population size estimation respectively,
    \item \code{offset} -- a matrix of offset values with number of columns matching the number of distribution parameters providing offset values to each of linear predictors.
\end{itemize}

With the \code{formula, data, model} being the three arguments which
must be provided in \code{estimatePopsize} syntax.

## Example with R code 

The package can be installed in a standard way using:

```{r, installation, eval=FALSE}
install.packages("singleRcapture")
```

To show the main function let us recreate the zero-truncated Poisson
model from \cite{ztpoisson} on the same data included in the package
under the name \code{netherlandsimmigrant}:

```{r, loading_singleRcapture, results='asis'}
library(singleRcapture)
knitr::kable(head(netherlandsimmigrant))
```

This data set contains information about immigrants in four cities
(Amsterdam, Rotterdam, The Hague and Utrecht) in Netherlands that have
been staying in the country without a legal permit in 1995 and have appeared in
police records that year. The number of times each individual appeared
in the records is included in the \code{capture} variable with the
available covariates being \code{gender, age, reason, nation} being
respectively the persons gender and age, reason for being captured and
region of the world from which each person comes:

```{r, netherlandsimmigrant_summary}
summary(netherlandsimmigrant)
```

One point which we should make while analysing this data set is that
there is a disproportionate number of individuals who were observed only
once:

```{r, table_netherlands, results='asis'}
knitr::kable(t(table(netherlandsimmigrant$capture)))
```

The basic syntax is vary similar to that of \code{glm} with the
output of the summary method being also quite similar except for the
additional results of the population size estimates:

```{r, basic_model_netherlands}
basicModel <- estimatePopsize(
  formula = capture ~ gender + age + nation,
  model   = ztpoisson(),
  data    = netherlandsimmigrant
)
summary(basicModel)
```
According to this simple model the population size is about 12.5k with about 15% of units observed in the register. The 95% CI under normality indicate that the true population size may be between 7k-18k with about 10% to 26% observed in the register.

Since there is a reasonable suspicion that the act of observing a unit
in the dataset may led to undesirable consequences from the point of
view of the subject of the observation (here possible deportation,
detainment or similar). For those reason researcher may consider one-inflated models such as \code{oiztgeom} and presented below.

```{r, model_inflated_netherlands, warning=FALSE, cache=T}
set.seed(123456)
modelInflated <- estimatePopsize(
    formula = capture ~ nation,
    model   = oiztgeom(omegaLink = "cloglog"),
    data    = netherlandsimmigrant,
    controlModel = controlModel(
        omegaFormula = ~ gender + age
    ),
    popVar = "bootstrap",
    controlPopVar = controlPopVar(bootType = "semiparametric")
)
summary(modelInflated)
```

This approach suggest that the population size is about 7k which is about 5k less than the naive Poisson approach. Comparison of AIC and BIC suggest that the one-inflation model fits the data better with BIC for \code{oiztgeom} `r round(BIC(modelInflated))` and `r round(BIC(basicModel))` for \code{ztpoisson}.

## Testing marginal frequencies

A popular method of testing the model fit in single source
capture-recapture studies is comparing the fitted marginal frequencies
$\displaystyle\sum_{j=1}^{N_{obs}}\hat{\mathbb{P}}\left[Y_{j}=k|\bx_{j}, Y_{j} > 0\right]$
with the observed marginal frequencies
$\displaystyle\sum_{j=1}^{N}\1_{\{k\}}(Y_{k})=\sum_{j=1}^{N_{obs}}\1_{\{k\}}(Y_{k})$
for $k\geq1$. If a fitted model bears sufficient resemblance to the real
data collection process these quantities should be quite close and both
$G$ and $\chi^{2}$ tests may be employed in order to test the
statistical significance of the discrepancy with the following
\pkg{singleRcapture} syntax for the Poisson model (rather poor fit):

```{r, marginal_freq_basic_model}
margFreq <- marginalFreq(basicModel)
summary(margFreq, df = 1, dropl5 = "group")
```

and for the one-inflated model (better fit):

```{r, marginal_freq_inflated_model}
margFreq_inf <- marginalFreq(modelInflated)
summary(margFreq_inf, df = 1, dropl5 = "group")
```

where the \code{dropl5} argument is used to indicate how to handle the
cells with less than $5$ fitted observations, note however that
currently there is no continuity correction. 

## Diagnostics

The \code{singleRStaticCountData} class has a \code{plot} method
implementing several types of quick demonstrative plots such as the
rootogram \citep[cf.][]{rootogram} for comparing the fitted and marginal
frequencies which we can get with the syntax:

```{r, rootogram, fig.show='hold', out.width="7.5cm"}
plot(   basicModel, plotType = "rootogram", main = "ZT Poisson model")
plot(modelInflated, plotType = "rootogram", main = "OI ZT Geometric model")
```

```{r, dev_off-1, include=FALSE}
dev.off()
```

Plots suggest that the \code{otztgeom} model fits the data better. Furthermore, important issue in population size estimation is the diagnostics of the models in order to verify whether influential observations are present in the data. For this purpose leave-one-out (LOO) diagnostic implemented in the \code{dfbeta} from the \pkg{stats} package was adapted and demonstrated below:

```{r, dfbeta_basic_model, results='asis'}
dfb <- dfbeta(basicModel)
knitr::kable(as.data.frame(
  t(apply(dfb, 2, quantile)*100)), digits = 4
)
```

Furthermore, result of the \code{dfbeta} can be further used in the function \code{dfpopsize} which allows for quantification of LOO on the population size.

```{r, dfpopsize_basic_model, results='asis'}
dfp <- dfpopsize(basicModel, dfbeta = dfb)
knitr::kable(as.data.frame(t(matrix(
  summary(dfp), dimnames = list(attr(summary(dfp), "names"), 1)
))))
```

The comparison of deletion effect on population size estimate and inverse probability weights, which refer to the contribution of a given observation to the population size estimation, is presented in the Figure bellow:

```{r, dfpopsize_plot}
plot(basicModel, plotType = "dfpopContr", dfpop = dfp)
```

This plot informs on the change of the population size if a given observation will be removed. For instance if we remove observation `r which.min(dfp)` from the data then population size will rise by about `r abs(round(min(dfp)))`.

The full list of plot types along with the list of optional arguments
which may be passed from the call to the \code{plot} method down to base
\proglang{R} and \pkg{graphics} functions is listed in the help file

```{r, eval=FALSE}
?plot.singleRStaticCountData
```

## The stratifyPopsize method

Researchers may be interested on only in the total population size but also in specific subpopulations (e.g. males, females, group pages). For that reason we have created function \code{stratifyPopsize} which allows to estimate the size by stratas defined by the coefficients in the model (the default option).


```{r, stratas, results='asis'}
popSizeStratas <- stratifyPopsize(basicModel)

cols <- c("name", "Observed", "Estimated", "logNormalLowerBound", 
          "logNormalUpperBound")
cols_custom <- c("Name", "Obs", "Estimated", "LowerBound", "UpperBound")

knitr::kable(popSizeStratas[, cols],  col.names = cols_custom, digits = 2)
```

One may also specify \code{plotType = "strata"} in the \code{plot} function which results in a plot with point and CI estimates of the population size.

```{r, strata_plot}
par(mar = c(2.5, 8.5, 4.1, 2.5), cex.main = .7, cex.lab = .6)
plot(basicModel, plotType = "strata")
```


The method for \code{singleRStaticCountData} class accepts three optional parameters
\code{stratas, alpha, cov} which correspond to specification of sub
populations, the significance levels and the covariance matrix that will
be used to compute standard errors. An example of the full call is presented below.

```{r, custom-strata, results='asis', message=F, warning=F}
library(sandwich)
popSizeStratasCustom <- stratifyPopsize(
  object  = basicModel,
  stratas = ~ gender / (nation + age), 
  alpha   = rep(c(.1, .2, .3, .4, .5), length.out = 18),
  cov     = vcovHC(basicModel, type = "HC4")
)

knitr::kable(popSizeStratasCustom[, cols], col.names = cols_custom, digits=2)
```

\normalsize

We provide integration with the \pkg{sandwich} \citep{sandwich} package to correct variance-covariance matrix in the $\delta$ method. In the code we have used the \code{vcovHC} method for \code{singleRStaticCountData}
class from the \pkg{sandwitch} package, different significance levels
for confidence intervals in each strata and a formula to specify that we
wanted estimates for both males and females subdivided by \code{nation}
and \code{age}. The \code{stratas} parameter may be specified either as:

\begin{itemize}
\item a formula with empty left hand side which we have seen here,
\item a logical vector with number of entries equal to number of rows in the dataset in which case only one strata will be created,
\item a (named) list where each element is a logical vector, names of the list will be used to specify names variable in returned object,
\item a vector of names of explanatory variables which will result in every level of explanatory variable having its own sub population for each variable specified,
\item or not supplied at all in which case stratas will correspond to levels of each factor in the data without any interactions (string vectors will be converted to factors for the convenience of the user).
\end{itemize}

For plotting only the \code{logNormal} type of confidence interval is
used since the studentized confidence intervals often result in negative
lower bounds.

## Implementation of Variance estimation

The package implements analytic and bootstrap variance estimators. In the control 
function \code{controlPopVar} user may specify the \code{bootType} argument which has
three possible values \code{"parametric", "semi-parametric"} and \code{"nonparametric"} 
Additional arguments accepted by the \code{contorlPopVar} function which
are relevant to bootstrap are:

\begin{itemize}
  \item \code{alpha, B} -- significance level and number of bootstrap samples to be performed respectively with $0.05$ and $500$ being the default options.
  \item \code{cores} -- number of process cores to use in bootstrap (1 by default) parallel computing is done via \pkg{doParallel, foreach, parallel} packages.
  \item \code{keepbootStat} --  logical value indicating whether to keep a vector of statistics produced by bootstrap.
  \item \code{traceBootstrapSize, bootstrapVisualTrace} --  logical values indicating whether sample and population size should be tracked (\code{FALSE} by default) these work only when \code{cores} = 1.
    \item \code{fittingMethod, bootstrapFitcontrol} -- fitting method (by default the same as used in the original call) and control parameters (\code{controlMethod}) for model fitting in bootstrap.
\end{itemize}

## Methods

For the purpose of the package we have created classes \code{singleRStaticCountData}, \code{singleR} (for now the two former classes are the same, the distinction is made for future development), \code{singleRfamily}, \code{popSizeEstResults}, \code{summarysingleRStaticCountData} and \code{summarysingleRmargin} which allows for extracting relevant information regarding the population size. For instance, function \code{popSizeEst} allows to extract information on the estimated size of the population as given below:

```{r, popsize_extract}
(popEst <- popSizeEst(basicModel))
```

and the resulting object \code{popEst} is of the \code{popSizeEstResults} class contains the following fields:

\begin{itemize}
  \item \code{pointEstimate}, \code{variance} -- numerics containing point estimate and variance of this estimate.
  \item \code{confidenceInterval} -- a \code{data.frame} with confidence intervals.
  \item \code{boot} -- If bootstrap was performed a numeric vector containing the $\hat{N}$ values from the bootstrap, 
  a character vector with value \code{"No bootstrap performed"} otherwise.
  \item \code{control} -- a \code{controlPopVar} object with controls used to obtained the object.
\end{itemize}

The only explicitly defined method for \code{popSizeEstResults}, \code{summarysingleRmargin} and \code{summarysingleRStaticCountData} classes is the print method, but the former one also accepts \proglang{R} primitives like \code{coef}:

```{r}
coef(summary(basicModel))
```

analogously to \code{glm} from \pkg{stats}. The \code{singleRfamily} inherits the \code{family} class from \pkg{stats} and has explictly defined \code{print} and \code{simulate} methods defined. Example usage:

```{r}
set.seed(1234567890)
N <- 10000
gender <- rbinom(N, 1, 0.2)
eta <- -1 + 0.5*gender
counts <- simulate(ztpoisson(), eta = cbind(eta), seed = 1)
summary(data.frame(gender, eta, counts))
```

The full list of explicitly defined methods for \code{singleRStaticCountData} methods is:

\begin{itemize}
  \item \code{AIC, BIC, extractAIC, family, confint, df.residual, model.frame, hatvalues, nobs, print} -- 
  Which work exactly like \code{glm} counterparts.
  \item \code{fitted} -- Which work almost exactly like \code{glm} counterparts but return more information, namely on fitted values for the truncated and non-truncated probability distribution.
  \item \code{logLik} -- which compared to \code{glm} method has the possiblity of returning not just the value of the fitted loglikelihood mut also the entire function (argument \code{type = "function"}) along with two first derivatives (argument \code{deriv = 0:2}).
  \item \code{model.matrix} -- which has the possibility of returning the $X_{\text{vlm}}$ matrix defined in \ref{X_vlm-definition}.
  \item \code{simulate} -- which calls \code{simulate} method for the chosen model and fitted $\boeta$.
  \item \code{predict} -- which has the possibility of returning either of fitted ditribution parameters for each unit (\code{type = "response"}), just linear predictors (\code{type = "link"}), means of the fitted distributnios of $Y$ and $Y|Y>0$ (\code{type = "mean"}) and the inverse probability weights (\code{type = "contr"}). There us also the \code{se.fit} argument which can be set to \code{TRUE} to obtain standard errors for each of those by using the $\delta$ method. Also it is possible to use a custom covariance matrix for standard error computation (argument \code{cov}).
  \item \code{redoPopEstimation} -- A function that applies all post-hoc procedures that were taken (such as heteroscedastic consistent covariance matrix estimation or bias reduction) to population size estimation and standard error estimation.
  \item \code{residuals} -- for obtaining residuals of several types, we refer interested readers to the manual \code{?singleRcapture:::residuals.singleRStaticCountData}.
  \item \code{stratifyPopsize, summary} -- which were already discussed. Compared to \code{glm} class summary has the possibility of adding confidence interval to the coefficient matrix (argument \code{confint = TRUE}) and using custom covariance matrix (argument \code{cov = someMatrix}).
  \item \code{plot} -- which was already discussed.
  \item \code{popSizeEst} -- an extractor showcased above.
  \item \code{cooks.distance} -- which works only for single predictor models
  \item \code{dfbeta, dfpopsize} -- Multithreading in \code{dfbeta} is available and \code{dfpopsize} calls \code{dfbeta} if no \code{dfbeta} object was provided at call.
  \item \code{bread, estfun, vcovHC} -- for (almost) full \pkg{sandwitch} compatibility.
\end{itemize}


# Integration with the \pkg{VGAM, countreg} packages {#VGAMcountreg-packages short-title="Integration with the"}

```{=html}
<!---
disadv singleRcapture w stosunku do VGAM to to że nie ma macierzy ograniczeń ogólnych, nie ma modeli addytywnych. i część VGAM jest zaprogramowana w C

adv to to że nie trzeba manualnie liczyć wariancji, jest bootstrap i to 3 rodzaje zrobione pod SSCR (w vgam nie ma w ogóle trzeba samemu) nie ma rootogramów porównywania rozkładów brzegowych zaobserwowanych i dopasowanych itp i przez to że nie ma ogólnych macierzy ograniczeń i zamiast tego mamy formuły na omega i alpha i pi to łatwiej jest wyspecyfikować, mamy też więcej rozkładów zero truncated (z wyjątkiem tego GAITD)

a z różnic jest to że Yee używa do obliczania macierzy informacji fishera symulacji monte carlo ja używam normalnego deterministycznego algorytmu do obliczania wartości szeregów i różnice w sumie nie są aż tak bardzo istotnie bo istenieje singleRcaptureExtra

-->
```

As noted at the beginning we provide an integration with the \pkg{VGAM} and \pkg{countreg} packages via the \pkg{singleRcaptureExtra} package available through Github at \url{https://github.com/ncn-foreigners/singleRcaptureExtra}. 


```{r, eval = FALSE}
install.packages("pak")
pak::pak("ncn-foreigners/singleRcaptureExtra")
```

The \pkg{singleRcaptureExtra} allows for converting objects
created by \code{vglm, vgam, countreg} functions from packages
\pkg{VGAM, countreg} to a \code{singleRStaticCountData} via the
respective \code{estimatePopsize} methods for their classes. The help
files for all the methods and all the control functions are accessed
by

```{r, eval=FALSE}
?estimatePopsize.vgam
?controlEstPopVgam
```

Using the fitted \code{zerotrunc, vglm, vgam} class objects in
population size estimation such as the one additive models with smooth
terms for dataset from \cite{chao-generalization}.

```{r, modelVGAM, warning=FALSE, message=FALSE, cache=T}
library(VGAM)
library(singleRcaptureExtra)
modelVgam <- vgam(
  TOTAL_SUB ~ (s(log_size, df  = 3) + s(log_distance, df  = 2)) / C_TYPE,
  data = farmsubmission,
  # Using different link since
  # VGAM uses parametrisation with 1/alpha
  family = posnegbinomial(
    lsize = negloglink
  )
)
```

Estimation of the population size can be accomplished with the following syntax simple syntax. 

```{r, singleRcaptureExtra_showcase, cache=TRUE}
modelVgamPop <- estimatePopsize(modelVgam)
```
The resulting object is of class \code{singleRforeign} to underline that the parameters were estimated outside the \pkg{singleRcapture}. The structure of the object is as follows

```{r}
str(modelVgamPop,1)
```


Compare with a similar linear model from base \pkg{singleRcapture}:
\small

```{r, singleRcaptureExtra_compare, cache=TRUE}
modelBase <- estimatePopsize(
  TOTAL_SUB ~ (log_size + log_distance) * C_TYPE,
  data = farmsubmission,
  model = ztnegbin()
)
summary(modelBase)
summary(modelVgamPop)
```

\normalsize

# Concluding remarks

Package \pkg{singleRcapture} 

... something more on the conclusions

# Acknowledgements {#Acknowledgements}

The authors' work has been financed by the National Science Centre in Poland, OPUS 20, grant no. 2020/39/B/HS4/00941. 

```{=html}
<!---
To był dla mnie prawie jedyny feedback jak pracowałem więc myśle że można ją dodać
-->
```

The authors would like to thank Layna Dennett from University of Southampton for usefull comments that led to the improved of the functionaly of the package.

\appendix

# Detailed information {#sec-details}

## The \code{estimatePopsizeFit} function {#estimatePopsizeFit-function short-title="The estimatePopsizeFit function"}

```{r, estimatePopsizeFit}
X <- matrix(data = 0, nrow = 2 * NROW(farmsubmission), ncol = 7)
X[1:NROW(farmsubmission), 1:4] <- model.matrix(
  ~ 1 + log_size + log_distance + C_TYPE, 
  farmsubmission
)
X[-(1:NROW(farmsubmission)), 5:7] <- X[1:NROW(farmsubmission), c(1, 3, 4)]
# this attribute tells the function which elements of the design matrix 
# correspond to which linear predictor 
attr(X, "hwm") <- c(4, 3)
start <- glm.fit(# get starting points
  y = farmsubmission$TOTAL_SUB, 
  x = X[1:NROW(farmsubmission), 1:4], 
  family = poisson()
)$coefficients
res <- estimatePopsizeFit(
  y            = farmsubmission$TOTAL_SUB, 
  X            = X, 
  method       = "IRLS", 
  priorWeights = 1, 
  family       = ztoigeom(), 
  control      = controlMethod(silent = TRUE), 
  coefStart    = c(start, 0, 0, 0),
  etaStart     = matrix(X %*% c(start, 0, 0, 0), ncol = 2),
  offset       = cbind(rep(0, NROW(farmsubmission)), 
                       rep(0, NROW(farmsubmission)))
)# extract results
ll <- ztoigeom()$makeMinusLogLike(y = farmsubmission$TOTAL_SUB, X = X)
print(c(res$beta, -ll(res$beta), res$iter))
# Compare with optim call
res2 <- estimatePopsizeFit(
  y = farmsubmission$TOTAL_SUB, 
  X = X, 
  method = "optim", 
  priorWeights = 1, 
  family = ztoigeom(), 
  coefStart = c(start, 0, 0, 0),
  control = controlMethod(silent = TRUE),
  offset = cbind(rep(0, NROW(farmsubmission)), rep(0, NROW(farmsubmission)))
)# extract results
c(res2$beta, -ll(res2$beta), res2$iter)
```

## Structure of a family function

```{=tex}
\itemize{
  \item \code{makeMinusLogLike} -- A factory function for creating the:
  \begin{equation*}
    \ell(\bbeta), 
    \frac{\partial\ell}{\partial\bbeta},
    \frac{\partial^{2}\ell}{\partial\bbeta^\top\partial\bbeta}
  \end{equation*}
  functions from $\boldsymbol{y}$ vector and $\bX_{vlm}$ the argument \code{deriv} with possible 
  values in \code{c(0, 1, 2)} provides which derivative to return with the default \code{0} being just the minus log-likelihood.
  \item \code{links} -- List with link functions.
  \item \code{mu.eta, variance} -- Functions of linear predictors that return expected value and variance. There is a `type` argument with 2 possible values \code{"trunc"} and \code{"nontrunc"} that specifies whether to return $\mathbb{E}[Y|Y>0], \text{var}[Y|Y>0]$ or $\mathbb{E}[Y], \text{var}[Y]$ respectively, also the \code{deriv} argument with values in \code{c(0, 1, 2)} is used for indicating the derivative with respect to the linear predictors with is used for providing standard error in \code{predict} method.
  \item \code{family} -- Character that specifies name of the model.
  \item \code{valideta, validmu} -- For now only returns true. In near future will be used to check whether applied linear predictors are valid (i.e. are transformed into some elements of parameter space the subjected to inverse link function).
  \item \code{funcZ, Wfun} -- Functions that create pseudo residuals and working weights used in IRLS algorithm.
  \item \code{devResids} -- Function that given the linear predictors prior weights vector and response vector returns deviance residuals.
  \item \code{pointEst, popVar} -- Functions that given prior weights linear predictors and in the later case also estimation of  $\text{cov}(\hat{\bbeta})$ and $\boldsymbol{X_{vlm}}$ matrix return point estimate for population size and analytic estimation of its variance.There is a additional boolean parameter \code{contr} in the former function that if set to true returns contribution of each unit.
  \item \code{etaNames} -- Names of linear predictors.
  \item \code{densityFunction} -- A function that given linear predictors returns value of PMF at values \code{x}. Additional argument \code{type} specifies whether to return $\mathbb{P}[Y|Y>0]$ or $\mathbb{P}[Y]$.
  \item \code{simulate} -- A function that generates values of dependent  vector given linear predictors.
  \item \code{getStart} -- Expression for generating starting points.
}
```

# Implementing custom \pkg{singleRcapture} family function {short-title="Implementing custom singleRcapture family function"}

Suppose we want to implement a very specific zero truncated family
function in the \pkg{singleRcapture} which corresponds to the following
"untruncated" distribution: \begin{equation}
  \mathbb{P}[Y=y|\lambda, \pi] = \begin{cases}
    1 - \frac{1}{2}\lambda - \frac{1}{2}\pi & \text{when: } y=0\\
    \frac{1}{2}\pi & \text{when: } y=1\\
    \frac{1}{2}\lambda & \text{when: } y=2,
  \end{cases}
\end{equation} with $\lambda, \pi\in\left(0, 1\right)$ being dependent
on covariates. The following would be one way of implementing it, with
\code{lambda, pi} in the code meaning
$\frac{1}{2}\lambda,\frac{1}{2}\pi$ in the equation above:

\footnotesize

```{r, family_function}
myFamilyFunction <- function(lambdaLink = c("logit", "cloglog", "probit"),
                             piLink     = c("logit", "cloglog", "probit"),
                             ...) {
  if (missing(lambdaLink)) lambdaLink <- "logit"
  if (missing(piLink))         piLink <- "logit"
  
  links <- list()
  attr(links, "linkNames") <- c(lambdaLink, piLink)
  
  lambdaLink <- switch(lambdaLink,
    "logit"   = singleRcapture:::singleRinternallogitLink,
    "cloglog" = singleRcapture:::singleRinternalcloglogLink,
    "probit"  = singleRcapture:::singleRinternalprobitLink
  )
  
  piLink <- switch(piLink,
    "logit"   = singleRcapture:::singleRinternallogitLink,
    "cloglog" = singleRcapture:::singleRinternalcloglogLink,
    "probit"  = singleRcapture:::singleRinternalprobitLink
  )
  
  links[1:2] <- c(lambdaLink, piLink)
  
  mu.eta <- function(eta, type = "trunc", deriv = FALSE, ...) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    
    if (!deriv) {
      switch (type,
        "nontrunc" = pi + 2 * lambda,
        "trunc" = 1 + lambda / (pi + lambda)
      )
    } else {
      # Only necessary if one wishes to use standard errors in predict method
      switch (type,
        "nontrunc" = {
          matrix(c(2, 1) * c(
            lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2,
                piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2
          ), ncol = 2)
        },
        "trunc" = {
          matrix(c(
            pi / (pi + lambda) ^ 2,
            -lambda / (pi + lambda) ^ 2
          ) * c(
            lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2,
                piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2
          ), ncol = 2)
        }
      )
    }
  }
  
  variance <- function(eta, type = "nontrunc", ...) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    
    switch (type,
    "nontrunc" = pi * (1 - pi) + 4 * lambda * (1 - lambda - pi),
    "trunc" = lambda * (1 - lambda) / (pi + lambda)
    )
  }
  
  Wfun <- function(prior, y, eta, ...) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    
    G01 <- ((lambda + pi) ^ (-2)) * piLink(eta[, 2], inverse = TRUE, deriv = 1) *
      lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) * prior / 4
    
    G00 <- ((lambda + pi) ^ (-2)) - (pi ^ (-2)) - lambda / ((lambda + pi) * (pi ^ 2))
    G00 <- G00 * prior * (piLink(eta[, 2], inverse = TRUE, deriv = 1) ^ 2) / 4
    
    G11 <- ((lambda + pi) ^ (-2)) - (((lambda + pi) * lambda) ^ -1)
    G11 <- G11 * prior * (lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) ^ 2) / 4
    
    matrix(
      -c(G11, # lambda
         G01, # mixed
         G01, # mixed
         G00  # pi
      ),
      dimnames = list(rownames(eta), c("lambda", "mixed", "mixed", "pi")),
      ncol = 4
    )
  }
  
  funcZ <- function(eta, weight, y, prior, ...) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    
    weight <- weight / prior
    
    G0 <- (2 - y) / pi     - ((lambda + pi) ^ -1)
    G1 <- (y - 1) / lambda - ((lambda + pi) ^ -1)
    
    G1 <- G1 * lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2
    G0 <- G0 *     piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2
    
    uMatrix <- matrix(c(G1, G0), ncol = 2)
    
    weight <- lapply(X = 1:nrow(weight), FUN = function (x) {
      matrix(as.numeric(weight[x, ]), ncol = 2)
    })
    
    pseudoResid <- sapply(X = 1:length(weight), FUN = function (x) {
      #xx <- chol2inv(chol(weight[[x]])) # less computationally demanding
      xx <- solve(weight[[x]]) # more stable
      xx %*% uMatrix[x, ]
    })
    pseudoResid <- t(pseudoResid)
    dimnames(pseudoResid) <- dimnames(eta)
    pseudoResid
  }
  
  minusLogLike <- function(y, X, offset,
                           weight    = 1, 
                           NbyK      = FALSE, 
                           vectorDer = FALSE, 
                           deriv     = 0, 
                           ...) {
    y <- as.numeric(y)
    if (is.null(weight)) {
      weight <- 1
    }
    if (missing(offset)) {
      offset <- cbind(rep(0, NROW(X) / 2), rep(0, NROW(X) / 2))
    }
    
    if (!(deriv %in% c(0, 1, 2))) stop("Only score function and derivatives up to 2 are supported.")
    deriv <- deriv + 1 # to make it conform to how switch in R works, i.e. indexing begins with 1
    
    switch (deriv,
      function(beta) {
        eta <- matrix(as.matrix(X) %*% beta, ncol = 2) + offset
        pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
        lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
        -sum(weight * ((2 - y) * log(pi) + (y - 1) * log(lambda) - log(pi + lambda)))
      },
      function(beta) {
        eta <- matrix(as.matrix(X) %*% beta, ncol = 2) + offset
        pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
        lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
        
        G0 <- (2 - y) / pi     - ((lambda + pi) ^ -1)
        G1 <- (y - 1) / lambda - ((lambda + pi) ^ -1)
        
        G1 <- G1 * weight * lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2
        G0 <- G0 * weight *     piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2
        
        if (NbyK) {
          XX <- 1:(attr(X, "hwm")[1])
          return(cbind(as.data.frame(X[1:nrow(eta), XX]) * G1, as.data.frame(X[-(1:nrow(eta)), -XX]) * G0))
        }
        if (vectorDer) {
          return(cbind(G1, G0))
        }
        
        as.numeric(c(G1, G0) %*% X)
      },
      function (beta) {
        lambdaPredNumber <- attr(X, "hwm")[1]
        eta <- matrix(as.matrix(X) %*% beta, ncol = 2) + offset
        pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
        lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2

        res <- matrix(nrow = length(beta), ncol = length(beta), 
                      dimnames = list(names(beta), names(beta)))
        
        # pi^2 derivative
        dpi <- (2 - y) / pi - (lambda + pi) ^ -1
        G00 <- ((lambda + pi) ^ (-2)) - (2 - y) / (pi ^ 2)
        
        G00 <- t(as.data.frame(X[-(1:(nrow(X) / 2)), -(1:lambdaPredNumber)] * 
        (G00 * ((piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2) ^ 2) + 
        dpi * piLink(eta[, 2], inverse = TRUE, deriv = 2) / 2) * weight)) %*% 
        as.matrix(X[-(1:(nrow(X) / 2)), -(1:lambdaPredNumber)])
        # mixed derivative
        G01 <- (lambda + pi) ^ (-2)
        
        G01 <- t(as.data.frame(X[1:(nrow(X) / 2), 1:lambdaPredNumber]) * 
        G01 * (lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2) * 
        (piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2) * weight) %*% 
        as.matrix(X[-(1:(nrow(X) / 2)), -(1:lambdaPredNumber)])
        # lambda^2 derivative
        G11 <- ((lambda + pi) ^ (-2)) - (y - 1) / (lambda ^ 2)
        dlambda <- (y - 1) / lambda - ((lambda + pi) ^ -1)
        
        G11 <- t(as.data.frame(X[1:(nrow(X) / 2), 1:lambdaPredNumber] * 
        (G11 * ((lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2) ^ 2) + 
        dlambda * lambdaLink(eta[, 1], inverse = TRUE, deriv = 2) / 2) * weight)) %*% 
        X[1:(nrow(X) / 2), 1:lambdaPredNumber]
        
        res[-(1:lambdaPredNumber), -(1:lambdaPredNumber)] <- G00
        res[1:lambdaPredNumber, 1:lambdaPredNumber] <- G11
        res[1:lambdaPredNumber, -(1:lambdaPredNumber)] <- t(G01)
        res[-(1:lambdaPredNumber), 1:lambdaPredNumber] <- G01
        
        res
      }
    )
  }
  
  validmu <- function(mu) {
    (sum(!is.finite(mu)) == 0) && all(0 < mu) && all(2 > mu)
  }
  
  # this is optional
  devResids <- function(y, eta, wt, ...) {
    0
  }
  
  pointEst <- function (pw, eta, contr = FALSE, ...) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    N <- pw / (lambda + pi)
    if(!contr) {
      N <- sum(N)
    }
    N
  }
  
  popVar <- function (pw, eta, cov, Xvlm, ...) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    
    bigTheta1 <- -pw / (pi + lambda) ^ 2 # w.r to pi
    bigTheta1 <- bigTheta1 * piLink(eta[, 2], inverse = TRUE, deriv = 1) / 2
    bigTheta2 <- -pw / (pi + lambda) ^ 2 # w.r to lambda
    bigTheta2 <- bigTheta2 * lambdaLink(eta[, 1], inverse = TRUE, deriv = 1) / 2# w.r to lambda
    
    bigTheta <- t(c(bigTheta2, bigTheta1) %*% Xvlm)
    
    f1 <- t(bigTheta) %*% as.matrix(cov) %*% bigTheta
    
    f2 <- sum(pw * (1 - pi - lambda) / ((pi + lambda) ^ 2))
    
    f1 + f2
  }
  
  dFun <- function (x, eta, type = c("trunc", "nontrunc")) {
    if (missing(type)) type <- "trunc"
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    
    switch (type,
      "trunc" = {
        (pi * as.numeric(x == 1) + lambda * as.numeric(x == 2)) / (pi + lambda)
      },
      "nontrunc" = {
        (1 - pi - lambda) * as.numeric(x == 0) +
        pi * as.numeric(x == 1) + lambda * as.numeric(x == 2)
      }
    )
  }
  
  simulate <- function(n, eta, lower = 0, upper = Inf) {
    pi     <-     piLink(eta[, 2], inverse = TRUE) / 2
    lambda <- lambdaLink(eta[, 1], inverse = TRUE) / 2
    CDF <- function(x) {
      ifelse(x == Inf, 1, 
      ifelse(x < 0, 0, 
      ifelse(x < 1, 1 - pi - lambda,
      ifelse(x < 2, 1 - lambda, 1))))
    }
    lb <- CDF(lower)
    ub <- CDF(upper)
    p_u <- stats::runif(n, lb, ub)
    sims <- rep(0, n)
    cond <- CDF(sims) <= p_u
    while (any(cond)) {
      sims[cond] <- sims[cond] + 1
      cond <- CDF(sims) <= p_u
    }
    sims
  }
  
  getStart <- expression(
    if (method == "IRLS") {
      etaStart <- cbind(
        family$links[[1]](mean(observed == 2) * (1 + 0 * (observed == 2))), # lambda
        family$links[[2]](mean(observed == 1) * (1 + 0 * (observed == 1)))  # pi
      ) + offset
    } else if (method == "optim") {
      init <- c(
        family$links[[1]](weighted.mean(observed == 2, priorWeights) * 1 + .0001),
        family$links[[2]](weighted.mean(observed == 1, priorWeights) * 1 + .0001)
      )
      if (attr(terms, "intercept")) {
        coefStart <- c(init[1], rep(0, attr(Xvlm, "hwm")[1] - 1))
      } else {
        coefStart <- rep(init[1] / attr(Xvlm, "hwm")[1], attr(Xvlm, "hwm")[1])
      }
      if ("(Intercept):pi" %in% colnames(Xvlm)) {
        coefStart <- c(coefStart, init[2], rep(0, attr(Xvlm, "hwm")[2] - 1))
      } else {
        coefStart <- c(coefStart, rep(init[2] / attr(Xvlm, "hwm")[2], attr(Xvlm, "hwm")[2]))
      }
    }
  )
  
  structure(
    list(
      makeMinusLogLike = minusLogLike,
      densityFunction  = dFun,
      links     = links,
      mu.eta    = mu.eta,
      valideta  = function (eta) {TRUE},
      variance  = variance,
      Wfun      = Wfun,
      funcZ     = funcZ,
      devResids = devResids,
      validmu   = validmu,
      pointEst  = pointEst,
      popVar    = popVar,
      family    = "myFamilyFunction",
      etaNames  = c("lambda", "pi"),
      simulate  = simulate,
      getStart  = getStart,
      extraInfo = c(
        mean       = "pi / 2 + lambda",
        variance   = paste0("(pi / 2) * (1 - pi / 2) + 2 * lambda * (1 - lambda / 2 - pi / 2)"),
        popSizeEst = "(1 - (pi + lambda) / 2) ^ -1",
        meanTr     = "1 + lambda / (pi + lambda)",
        varianceTr = paste0("lambda * (1 - lambda / 2) / (pi + lambda)")
      )
    ),
    class = c("singleRfamily", "family")
  )
}
```

\normalsize

A quick tests shows us that this implementation in fact works:

```{r, family_function_showcase}
set.seed(123)
Y <- simulate(
    myFamilyFunction(lambdaLink = "logit", piLink = "logit"),
    nsim = 1000, eta = matrix(0, nrow = 1000, ncol = 2),
    truncated = FALSE
)
mm <- estimatePopsize(
    formula = Y ~ 1,
    data = data.frame(Y = Y[Y > 0]),
    model = myFamilyFunction(lambdaLink = "logit", 
                             piLink = "logit"),
    # the usual observed information matrix 
    # is ill-suited for this distribution
    controlPopVar = controlPopVar(covType = "Fisher")
)
summary(mm)
```

Where the link functions such as
\code{singleRcapture:::singleRinternalcloglogLink} are just internal
functions in \pkg{singleRcapture} that compute link functions their
inverses and derivatives of both links and inverse link up to third
order: \small

```{r, link_function}
singleRcapture:::singleRinternalcloglogLink
```

\normalsize

one might of course include code for computing them manually.
